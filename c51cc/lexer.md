# Лексический анализатор (Lexer) для C51

## Введение

Лексический анализатор (также известный как лексер или токенизатор) - это первый этап в процессе компиляции, который преобразует исходный код из текстовой строки в последовательность токенов. Токены - это атомарные элементы языка, такие как ключевые слова, идентификаторы, числа, операторы и разделители.

В данном документе мы рассмотрим реализацию лексического анализатора для языка C51 (диалект C для микроконтроллеров 8051), написанного на языке Clojure.

## Структура лексера

Наш лексер состоит из нескольких ключевых компонентов:

1. **Объявление типов токенов** - определение всех возможных типов токенов в языке
2. **Функции проверки токенов** - функции для определения типа строки
3. **Функция токенизации** - основная функция для преобразования исходного кода в последовательность токенов
4. **Вспомогательные функции** - дополнительные функции для работы с токенами

### 1. Объявление типов токенов

```clojure
(def keywords
  {
   ;; Special Keywords    
   :sfr_special_keyword ["sfr"]
   :sbit_special_keyword ["sbit"]
   :interrupt_special_keyword ["interrupt"]
   :using_special_keyword ["using"]
   
   ;; Data Types
   :char_type_keyword ["char"]
   :int_type_keyword ["int"]
   :void_type_keyword ["void"]
   :signed_type_keyword ["signed"]
   :unsigned_type_keyword ["unsigned"]

   ;; Separators
   :open_round_bracket ["("]
   :close_round_bracket [")"]
   :open_curly_bracket ["{"]
   :close_curly_bracket ["}"]
   :open_square_bracket ["["]
   :close_square_bracket ["]"] 
   :semicolon [";"]
   :comma [","]
   :colon [":"]   

   ;; Operators
   :arithmetic_operators ["+", "-", "*", "/", "%"]
   :comparison_operators ["==", "!=", "<", ">", "<=", ">="]
   :logical_operators ["&&", "||", "!"]
   :bitwise_operators ["&", "|", "^"]
   :bitwise_shift_operators ["<<", ">>"]
   :assignment_operators ["=", "+=", "-=", "*=", "/=", "%=", "&=", "|=", "^="]
   :inc_operator ["++"]
   :dec_operator ["--"]
   :increment_decrement_operators ["++", "--"]
   :unary_operators ["~", "!"]

   ;; Control Flow:
   :if_keyword ["if"]
   :else_keyword ["else"]
   :switch_keyword ["switch"]
   :case_keyword ["case"]
   :default_keyword ["default"]
   :for_keyword ["for"]
   :while_keyword ["while"]
   :do_keyword ["do"]
   :break_keyword ["break"]
   :continue_keyword ["continue"]
   :return_keyword ["return"]
   :goto_keyword ["goto"]
   
   ;; Constants
   :const_keyword ["const"]

   ;; Регулярные выражения для токенов с переменной структурой
   :identifier #"[a-zA-Z_\p{L}][a-zA-Z0-9_\p{L}]*"
   :int_number #"[-+]?[0-9]+"   ;; Только целые числа, плавающая точка запрещена
   :hex_number #"0[xX][0-9a-fA-F]+"
   :string #"\"[^\"]*\""
   })
```

Здесь мы определяем все типы токенов, которые могут встретиться в коде C51. Каждый тип токена имеет свое уникальное имя (ключ) и список строк или регулярное выражение (значение), которые соответствуют этому типу.

Токены разделены на несколько категорий:
- **Специальные ключевые слова** - слова, специфичные для C51 (например, `sfr`, `sbit`)
- **Типы данных** - ключевые слова для объявления типов данных (например, `int`, `char`)
- **Разделители** - символы для разделения частей кода (например, `{`, `;`)
- **Операторы** - символы для выполнения операций (например, `+`, `==`)
- **Управляющие конструкции** - ключевые слова для управления потоком выполнения (например, `if`, `for`)
- **Константы** - ключевые слова для определения констант (например, `const`)
- **Идентификаторы, числа и строки** - определяются с помощью регулярных выражений

### 2. Функции проверки токенов

```clojure
(defn is-special-keyword? [s]
  (boolean (some #{s} 
                 (concat 
                  (get keywords :sfr_special_keyword)
                  (get keywords :sbit_special_keyword)
                  (get keywords :interrupt_special_keyword)
                  (get keywords :using_special_keyword)))))
```

Функции с префиксом `is-` используются для проверки, принадлежит ли строка к определенному типу токенов. Например, `is-special-keyword?` проверяет, является ли строка специальным ключевым словом.

**Пример:**
```clojure
(is-special-keyword? "sfr") ;; => true
(is-special-keyword? "int") ;; => false
```

Аналогичным образом реализованы другие функции для проверки разных типов токенов:
- `is-type-keyword?` - для типов данных
- `is-separator-keyword?` - для разделителей
- `is-operator-keyword?` - для операторов
- `is-control-flow-keyword?` - для управляющих конструкций
- `is-constant-keyword?` - для констант
- `is-identifier?` - для идентификаторов
- `is-number?` - для чисел
- `is-string?` - для строк

### 3. Ключевые функции токенизации

#### token-for-keyword

```clojure
(defn token-for-keyword [token keyword-type]
  "Создает токен для ключевого слова с указанным типом"
  {:value token :type keyword-type})
```

Эта функция создает структуру токена - ассоциативный массив с ключами `:value` (значение токена) и `:type` (тип токена).

**Пример:**
```clojure
(token-for-keyword "int" :type-keyword)
;; => {:value "int", :type :type-keyword}
```

#### create-token-map

```clojure
(defn create-token-map []
  "Создает карту токенов для быстрого сопоставления"
  (let [keyword-type-mapping {
        ;; Маппинг ключевых слов на типы токенов
        ...
        }]
    
    (->> (dissoc keywords :identifier :int_number :hex_number :string)
         (mapcat (fn [[k v]]
                   (when (vector? v)
                     (map #(vector % (token-for-keyword % (get keyword-type-mapping k k))) v))))
         (into {})
         ;; Сортируем по длине токена (от большего к меньшему) для корректного совпадения
         (sort-by #(- (count (first %)))))))
```

Эта функция создает карту (словарь), где ключами являются строковые представления токенов, а значениями - структуры токенов. Она выполняет следующие шаги:
1. Удаляет из `keywords` токены с регулярными выражениями
2. Преобразует каждый элемент в пару [строка, токен]
3. Преобразует список пар в ассоциативный массив
4. Сортирует массив по длине ключа в убывающем порядке

Сортировка по длине необходима для правильного сопоставления составных операторов. Например, оператор `==` должен быть проверен до оператора `=`.

**Пример результата:**
```clojure
{
  "==" {:value "==", :type :operator}
  "+=" {:value "+=", :type :operator}
  "=" {:value "=", :type :operator}
  "+" {:value "+", :type :operator}
  ;; ... другие токены
}
```

#### find-regex-token

```clojure
(defn find-regex-token [code]
  "Находит токен на основе регулярных выражений"
  (let [hex-match (re-find #"^0[xX][0-9a-fA-F]+" code)]
    (cond 
      ;; Проверяем сначала шестнадцатеричные числа, чтобы не перепутать с идентификаторами
      hex-match
      {:value hex-match
       :type :hex_number}
      
      ;; Проверяем целые числа (без плавающей точки)
      (re-find #"^[-+]?[0-9]+" code)
      {:value (re-find #"^[-+]?[0-9]+" code)
       :type :int_number}
      
      ;; Проверяем строки
      (and (str/starts-with? code "\"") 
           (> (count code) 1)
           (re-find #"^\"[^\"]*\"" code))
      {:value (re-find #"^\"[^\"]*\"" code)
       :type :string}
      
      ;; Проверяем идентификаторы
      (re-find #"^[a-zA-Z_\p{L}][a-zA-Z0-9_\p{L}]*" code)
      {:value (re-find #"^[a-zA-Z_\p{L}][a-zA-Z0-9_\p{L}]*" code)
       :type :identifier}
      
      :else nil)))
```

Эта функция пытается найти токен в начале строки, используя регулярные выражения. Она проверяет типы токенов в определенном порядке:
1. Шестнадцатеричные числа (например, `0xFA`)
2. Целые числа (например, `123`, `-456`)
3. Строковые литералы (например, `"Hello"`)
4. Идентификаторы (например, `variable_name`)

Порядок проверки важен, так как некоторые токены могут совпадать с начальными частями других токенов. Например, шестнадцатеричное число `0xFA` должно быть распознано как один токен, а не как число `0` и идентификатор `xFA`.

**Примеры:**
```clojure
(find-regex-token "0xFA") ;; => {:value "0xFA", :type :hex_number}
(find-regex-token "123") ;; => {:value "123", :type :int_number}
(find-regex-token "\"Hello\" + 5") ;; => {:value "\"Hello\"", :type :string}
(find-regex-token "variable = 10") ;; => {:value "variable", :type :identifier}
```

#### tokenize

```clojure
(defn tokenize [code]
  "Преобразует исходный код в последовательность токенов"
  (let [token-map (create-token-map)]
    (loop [remaining-code (str/trim code)
           tokens []]
      (if (str/blank? remaining-code)
        tokens
        (let [;; Сначала пытаемся найти точное совпадение токена
              exact-match (some (fn [[token token-info]]
                                  (when (str/starts-with? remaining-code token)
                                    token-info))
                                token-map)
              
              ;; Если точного совпадения нет, пытаемся найти по регулярным выражениям
              token (or exact-match (find-regex-token remaining-code))]
          
          (if token
            (let [value (:value token)
                  token-length (if (string? value) 
                                 (count value) 
                                 ;; Для случая, когда value не строка (возвращаемое значение из re-find может быть вектором)
                                 (count (first (if (vector? value) value [value]))))]
              (recur 
               (str/trim (subs remaining-code token-length))
               (conj tokens (if (vector? (:value token))
                              ;; Если значение - вектор, берем первый элемент (полное совпадение)
                              (assoc token :value (first (:value token)))
                              token))))
            (throw (ex-info "Tokenization error" {:remaining-code remaining-code}))))))))
```

Это основная функция лексического анализатора, которая преобразует исходный код в последовательность токенов. Она работает следующим образом:

1. Создает карту токенов с помощью `create-token-map`
2. Инициализирует пустой список токенов
3. В цикле обрабатывает оставшийся код:
   - Если код пуст, возвращает список токенов
   - Иначе пытается найти токен в начале кода:
     - Сначала проверяет точное совпадение с ключевыми словами и операторами
     - Если не найдено, ищет по регулярным выражениям
   - Если токен найден, удаляет его из кода и добавляет в список токенов
   - Если токен не найден, генерирует исключение

Функция обрабатывает возможный случай, когда значение токена является вектором (результат работы `re-find`), и корректно извлекает первый элемент вектора (полное совпадение).

**Пример:**
```clojure
(tokenize "int x = 5 + 3;")
;; => [
;;      {:value "int", :type :type-keyword}
;;      {:value "x", :type :identifier}
;;      {:value "=", :type :operator}
;;      {:value "5", :type :int_number}
;;      {:value "+", :type :operator}
;;      {:value "3", :type :int_number}
;;      {:value ";", :type :separator}
;;    ]
```

## Процесс лексического анализа на примере

Рассмотрим процесс токенизации на примере простого кода C51:

```c
void main() {
    int counter = 0;
    while(counter < 10) {
        counter++;
    }
}
```

1. Начинаем с первого символа: `void`
   - Проверяем на точное совпадение с ключевыми словами и находим `void` в списке типов
   - Создаем токен `{:value "void", :type :type-keyword}`
   - Оставшийся код: ` main() { ... }`

2. Обрабатываем следующий элемент: `main`
   - Не находим в списке ключевых слов
   - Применяем регулярное выражение для идентификаторов и находим `main`
   - Создаем токен `{:value "main", :type :identifier}`
   - Оставшийся код: `() { ... }`

3. Обрабатываем следующий элемент: `(`
   - Находим в списке разделителей
   - Создаем токен `{:value "(", :type :separator}`
   - Оставшийся код: `) { ... }`

... и так далее для каждого элемента кода

В результате получаем список токенов:
```clojure
[
  {:value "void", :type :type-keyword}
  {:value "main", :type :identifier}
  {:value "(", :type :separator}
  {:value ")", :type :separator}
  {:value "{", :type :separator}
  {:value "int", :type :type-keyword}
  {:value "counter", :type :identifier}
  {:value "=", :type :operator}
  {:value "0", :type :int_number}
  {:value ";", :type :separator}
  {:value "while", :type :control-flow}
  {:value "(", :type :separator}
  {:value "counter", :type :identifier}
  {:value "<", :type :operator}
  {:value "10", :type :int_number}
  {:value ")", :type :separator}
  {:value "{", :type :separator}
  {:value "counter", :type :identifier}
  {:value "++", :type :operator}
  {:value ";", :type :separator}
  {:value "}", :type :separator}
  {:value "}", :type :separator}
]
```

## Особенности реализации

### 1. Обработка составных операторов

Операторы типа `==`, `+=` или `++` должны быть распознаны как единые токены, а не как последовательность символов. Для этого мы сортируем карту токенов по длине ключа в убывающем порядке. Это гарантирует, что более длинные операторы будут проверены перед короткими.

Например, мы сначала проверим `==` перед `=`, что предотвратит неправильное распознавание оператора сравнения.

### 2. Обработка шестнадцатеричных чисел и целых чисел

Шестнадцатеричные числа начинаются с `0x` или `0X`, за которыми следуют цифры от 0 до 9 и буквы от A до F. Мы проверяем эти числа в первую очередь, чтобы не перепутать их с идентификаторами.

Пример: `0xFA` должен быть распознан как шестнадцатеричное число, а не как число `0` и идентификатор `xFA`.

**Важно:** В языке C51 поддерживаются только целые числа. Числа с плавающей точкой (float, double) запрещены, поэтому наш лексер распознает только целые числа и шестнадцатеричные значения.

### 3. Обработка строк

Строки в C51 заключены в двойные кавычки. Наше регулярное выражение для строк учитывает эту особенность и корректно распознает строки, даже если они содержат пробелы или другие специальные символы.

### 4. Обработка возможных ошибок

Если лексический анализатор не может распознать токен, он генерирует исключение с информацией об ошибке, включая оставшийся код. Это помогает диагностировать проблемы в исходном коде.

## Заключение

Лексический анализатор - это первый шаг в процессе компиляции, который преобразует исходный код из текстовой строки в последовательность токенов. В нашей реализации мы использовали функциональный подход на языке Clojure, что делает код более понятным и поддерживаемым.

Основные особенности нашего лексического анализатора:
- Определение всех типов токенов в виде карты
- Функции для проверки типов токенов
- Эффективное распознавание токенов с использованием как точного сопоставления, так и регулярных выражений
- Правильная обработка составных операторов и специальных символов

Этот лексический анализатор является основой для следующих этапов компиляции: синтаксического анализа и генерации кода. 